{"meta":{"title":"Gold Experience","subtitle":"muda!","description":null,"author":"ZhangZhen","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Python爬取猫眼电影排行","slug":"Python爬取猫眼电影排行","date":"2019-08-06T11:58:10.000Z","updated":"2019-08-06T13:21:07.750Z","comments":true,"path":"2019/08/06/Python爬取猫眼电影排行/","link":"","permalink":"http://yoursite.com/2019/08/06/Python爬取猫眼电影排行/","excerpt":"Python爬取猫眼电影排行利用Requests库和正则表达式来抓取猫眼电影Top100的相关内容。requests库比urllib库更方便。","text":"Python爬取猫眼电影排行利用Requests库和正则表达式来抓取猫眼电影Top100的相关内容。requests库比urllib库更方便。 1.本次目标提取出猫眼电影Top100的电影名称，时间，评分，，图片等信息，提取的内容会以文件形式保存下来。 2.准备工作安装Requests库。 3.抓取分析目标网页猫眼电影，打开之后可以看到榜单信息排名第一的是《霸王别姬》页面中显示的有效信息有影片名称，主演，上映时间，上映地区，评分，图片等信息。将网页滚动到最下方，发现有分页的列表，直接点击第2页，观察页面的URL和内容发生了怎样的变化，如图此页面的URL变成https://maoyan.com/board/4?offset=10比之前的URL多了一个参数offset=10，而页面显示的是排行1120名的电影，初步推断这是一个偏移量参数。再点击下一页发现页面的URL中的参数offset变成了20，显示的结果也是2130名的电影。由此推断，offset代表偏移量，如果偏移量为n，则显示n+1~n+10，每页显示10个，所以想获取Top100只需要分开请求10次即可。 4.抓取第一页实现get_one_page()方法，并传入url参数，然后将抓取结果返回，通过mian()方法调用，代码如下： import requests def get_one_page(url): try: headers = { 'User-Agent':'Mozilla/5.0(Macintosh;Intel Mac Os X10_13_3)Applewebkit/537.36(KHTML,LikeGecko)Chrome/65.0.3325.162 Safari/537.36'} response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None def mian(): url = 'http://maoyan.com/board/4' html = get_one_page(url) print(html) main() 这一步运行之后，就可以获取首页的源代码了，下一步就是解析源代码，获取想要的信息。 5.正则提取F12进入开发者模式后，在network中查看源代码。注意这里不要再Elements选项卡中直接查看源码，因为那里的源码有可能经过JS处理而有所不同。通过上图可以看到一部电影信息对应一个节点，使用正则表达式来提取排名，图片，名称，主演，发布时间，评分等内容。此时正则表达式改写如下: &lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt; 最后通过findall()方法提取出所有内容。接下来，定义解析页面的方法parse_one_page(),主要是通过正则表达式来从结果中提取出所有的内容。将代码梳理一下，遍历提取结果并生成字典，实现代码如下: def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=\"(.*?)\".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;',re.S) items = re.findall(pattern,html) for item in items: yield{ 'index':item[0], 'image':item[1], 'title':item[2].strip(), 'actor':item[3].strip()[3:], 'time':item[4].strip()[5:], 'score':item[5].strip() + item[6] } 到此为止，我们已经成功的提取了单页的电影信息。 6.写入文件通过JSON库的dumps()方法实现字典的序列化，并指定ensure_ascii参数为false，这样可以保证输出结果是中文形式而不是Unicode编码。代码如下: def write_to_file(content): with open('result.txt','a',encoding = 'utf-8') as f: f.write(json.dumps(content,ensure_ascii = False)+'\\n') 7.整合代码最后实现main()方法来调用。 def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item) 8.分页爬取因为需要爬取Top100的电影，所以给这个链接传入offset参数，实现其他90部电影的爬取，此时添加如下调用即可: if __name__ == '__main__': for i in range(10): main(offset = i*10) time.sleep(1) 到此为止，我们的小爬虫就完成了，整理代码如下: import requests import re import json import time from requests.exceptions import RequestException def get_one_page(url): try: headers = { 'User-Agent':'Mozilla/5.0(Macintosh;Intel Mac Os X10_13_3)Applewebkit/537.36(KHTML,LikeGecko)Chrome/65.0.3325.162 Safari/537.36'} response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: return None def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=\"(.*?)\".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;',re.S) items = re.findall(pattern,html) for item in items: yield{ 'index':item[0], 'image':item[1], 'title':item[2].strip(), 'actor':item[3].strip()[3:], 'time':item[4].strip()[5:], 'score':item[5].strip() + item[6] } def write_to_file(content): with open('result.txt','a',encoding = 'utf-8') as f: f.write(json.dumps(content,ensure_ascii = False)+'\\n') def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item) if __name__ == '__main__': for i in range(10): main(offset = i*10) time.sleep(1) 运行结果如下：大功告成！ 10.本节代码本节代码地址为:https://github.com/JerryZmio/python_webspider本节是一个简单的实例，希望大家对爬虫的实现有一个基本的思路，也对正则的用法有更深一步的了解。","categories":[],"tags":[{"name":"爬虫，python，教程","slug":"爬虫，python，教程","permalink":"http://yoursite.com/tags/爬虫，python，教程/"}]},{"title":"","slug":"MarkDown语法笔记（一）","date":"2019-08-05T14:11:52.709Z","updated":"2019-08-06T13:11:42.762Z","comments":true,"path":"2019/08/05/MarkDown语法笔记（一）/","link":"","permalink":"http://yoursite.com/2019/08/05/MarkDown语法笔记（一）/","excerpt":"MarkDown语法笔记对MarkDown语法的总结，适合像我一样的小白观看。","text":"MarkDown语法笔记对MarkDown语法的总结，适合像我一样的小白观看。 强调斜体*斜体* 或者 _斜体_``` ### **粗体** 粗体 或者 粗体 ## 标题 需注意‘#’和标题间应有空格一级标题二级标题###三级标题 四级标题五级标题六级标题## 换行 行尾输入两个及以上空格，然后回车。或者行尾加上`&lt;br&gt;`。 ## 引用 被引用内容段落开口加上右尖括号`&gt;`即可。可以在开头加一个，也可以每行加一个，效果是一样的。 **效果：** &gt;例如： 愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。 有一分热，发一分光，就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。此后如竟没有炬火：我便是唯一的光。《热风》 愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。 有一分热，发一分光，就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。此后如竟没有炬火：我便是唯一的光。《热风》 引用可以**嵌套**。如果要在一个引用里插入一个引用，可以用两个&gt;开头。依此类推，根据嵌套层次加相应数量的符号 这是一个引用。这是第一行 这是第二行。 &gt; 这是一个嵌套的引用。这是第一行。 这是第二行 外层引用的第三行。前面需要一个视觉上的空行表示内层嵌套的结束，空行前面的&gt;可以有可以没有。 链接链接可以在行内插入： [链接文字](链接地址) [百度](www.baidu.com)百度 链接地址可以放在段落后面的脚注，前面放上链接引用标签区分。举例说，先在内容行内插入以下内容： [链接文字][链接引用标签] 然后在段落的后面（或者文档的结尾）放上以下内容，就可以生成一个链接： [链接引用标签]: http://zh.wikipedia.com/wiki/Markdown &quot;链接标题&quot; 水平分区线要生成水平分区线，可以在单独一行里输入3个或以上的短横线、星号或者下划线实现。短横线和星号之间可以输入任意空格。以下每一行都产生一条水平分区线。 * * * *** ***** - - - ---------------------------------------图片![Foo](http://i.weather.com.cn/images/cn/life/2017/04/11/11141533DF572FBBA092E37E6E843C656C318272.jpg) 列表无序列表 使用*，+，-表示无序列表 + 无序列表项 一 - 子无序列表 一 - 子无序列表 二 * 子无序列表 三 + 无序列表项 二 + 无序列表项 三 效果： 无序列表项 一 子无序列表 一 子无序列表 二 子无序列表 三 无序列表项 二 无序列表项 三 有序列表使用数字和点表示有序列表。 1. 有序列表项 一 1. 子有序列表项 一 2. 子有序列表项 二 2. 有序列表项 二 3. 有序列表项 三 有序列表项 一 子有序列表项 一 子有序列表项 二 有序列表项 二 有序列表项 三 代码`代码`代码块及代码高亮在4个`后加上语言类型，高亮显示代码，括号可去 (4个`)(语言类型) 代码 (4个`) 效果： #include stdio.h void main() { int i = 0; for(t = 0;t&gt;7;t++) { i++; } } 结语小白自学笔记系列。 时间有限，先写到这里。一周内应该会完成MarkDown语法笔记（二）。顺便开个新坑，一周内会写一篇常用开发工具和日常软件的分享。如有错误，欢迎指出。QQ:3497574618","categories":[],"tags":[]},{"title":"","slug":"开发工具分享","date":"2019-08-05T14:11:52.709Z","updated":"2019-08-05T14:35:02.730Z","comments":true,"path":"2019/08/05/开发工具分享/","link":"","permalink":"http://yoursite.com/2019/08/05/开发工具分享/","excerpt":"日常开发软件嵌入式","text":"日常开发软件嵌入式 51单片机 keil4做程序编写 没什么好说的，Windows开发51基本都用这个软件。就是没有代码补全太蛋疼。以及中文乱码，做12864显示调试实在有些心累。orz stc-isp烧写软件烧写hex文件，用STC芯片的都避不开,使用方法也很简单。 Atium Designer 17画板子的，软件还算好用，就是工程文件经常被我搞混。目前个人PCB和原理图库也积攒了一部分，有时间分享出来吧。 其他辅助工具像是串口调试工具，汉字和图片取模工具等，大约有7，8个 Labview 还在学习中，目前还只会写些简单的上位机程序。 Proteus8 做51仿真非常方便，网上大神做了很多元件。19年帮学姐做的一份毕设就用了ST7920的元件仿真。遗憾的是，目前还不能仿真语音芯片。上网找了一圈也没找到ISD芯片的仿真库orz这份毕设过段时间也会分享出来 STM32挖坑 * 2，待填 Python IDE Pycharm(community版) Python还在学习中，这个软件可以省去很多麻烦，环境配置简单，debug工具实用，代码高亮配色我挺烦喜欢，不知道算不算有点，hhh。 VS code 写Markdown，python和c的。优点是轻量化，打开速度快，功能丰富。支持40多种语言。 结语前两天重做了系统，很多软件丢失了，此贴会慢慢更新，把剩下的慢慢填进来。","categories":[],"tags":[]}]}